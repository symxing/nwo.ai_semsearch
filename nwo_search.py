# -*- coding: utf-8 -*-
"""nwo search

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18hRKoqkN8T2iiJ6NTIEGOE5ZbQomwiaw
"""

import os 
"""
all os environmental code listed below is for reading my credentials into 
Google Colab, so I can see what I'm doing 
 """
os.environ["GOOGLE_APPLICATION_CREDENTIALS"]="nwo-sample-5f8915fdc5ec.json"

#ensure the path is set correctly
!echo $GOOGLE_APPLICATION_CREDENTIALS

# Imports 

from google.cloud import bigquery
from google.oauth2 import service_account 
import csv
import pandas as pd 

# service account credentials
credentials = service_account.Credentials.from_service_account_file(
    "nwo-sample-5f8915fdc5ec.json"
)

client = bigquery.Client(credentials=credentials)

# Query to create a table with only the body text from the reddit table 
# and the tweets from the twitter table 

# QUERY = (
#     """
#     CREATE TABLE nwo-sample.graph.body_text AS 
#     (SELECT body
#     FROM `nwo-sample.graph.reddit`
#     UNION ALL
#     SELECT tweet
#     FROM `nwo-sample.graph.tweets`)
#     """
# )


# I will only grab the first 50,000 documents in order to make sure my code doesn't time out 
QUERY = (
    '''
    SELECT *
    FROM nwo-sample.graph.body_text
    LIMIT 50000
    '''
)

df = client.query(QUERY).to_dataframe()

# Save the query results to a csv to create my model on 
df.to_csv('words.csv', index=False)

# imports for my model
import numpy as np
import spacy
import string
import gensim
import operator
import re 

# turning the csv into a dataframe 
# adding the engine as python because this csv file is massive and the text 
# hasn't been cleaned yet 
df_text = pd.read_csv('words.csv', engine='python')
df_text.head()

# This section will clean a pre-process the documents 

from spacy.lang.en.stop_words import STOP_WORDS

spacy_nlp = spacy.load('en_core_web_sm')

# Creating a list of punctuation and stopwords
punctuations = string.punctuation
stop_words = spacy.lang.en.stop_words.STOP_WORDS

# Clean and process data 
def spacy_tokenizer(sentence):

  # removing digits 
  sentence = re.sub('\w*\d\w*','', sentence)

  # replacing extra spacing with single spacing 
  sentence = re.sub(' +',' ',sentence)

  # remove single quotes
  sentence = re.sub('\'','',sentence)

  # removing special characters 
  sentence = re.sub(r'\n: \'\'.*', '', sentence)
  sentence = re.sub(r'\n!.*','', sentence)
  sentence = re.sub(r'^:\'\'.*','', sentence)

  # remove punctuation
  sentence = re.sub(r'[^\w\s]',' ', sentence)

  # create token object
  tokens = spacy_nlp(sentence)

  # lower and lemmatize
  tokens = [word.lemma_.lower().strip() if word.lemma_ != "-PRON-" 
            else word.lower_ for word in tokens]

  # remove stopwords 
  tokens = [word for word in tokens if word not in stop_words 
            and word not in punctuations]

  return tokens

#remove special characters
df_text['clean_body'] = df_text['body'].str.replace("[^a-zA-Z#]", " ")
#remove words have letters less than 3
df_text['clean_body'] = df_text['clean_body'].fillna('').apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))
#lowercase all characters
df_text['clean_body'] = df_text['clean_body'].fillna('').apply(lambda x: x.lower())
df_text.head()

# Commented out IPython magic to ensure Python compatibility.
# Tokenize the cleaned text, with %time just so I can see how long it takes  

# %time df_text['clean_body'] = df_text['clean_body'].map(lambda x: spacy_tokenizer(x))

df_text.head()

post_tokens = df_text['clean_body']

# Next step is to build a word dictionary so we can sort the tokens appropriately

from gensim import corpora

# Creating dictionary 
dictionary = corpora.Dictionary(post_tokens)

# This is the fun part! I will be using an LSI (latent semantic indexing) model 

# Define the corpus 
corpus = [dictionary.doc2bow(desc) for desc in post_tokens]

#Build a tf-Idf to determine the most important words in the document 
text_tfidf_model = gensim.models.TfidfModel(corpus, id2word=dictionary)
text_lsi_model = gensim.models.LsiModel(text_tfidf_model[corpus], id2word=dictionary, num_topics=300)

# Store the corpus and serialize for easy referencing

gensim.corpora.MmCorpus.serialize('text_tfidf_model_mm', text_tfidf_model[corpus])
gensim.corpora.MmCorpus.serialize('text_lsi_model_mm', text_lsi_model[text_tfidf_model[corpus]])

# Load the indexed corpus and check the sizes

text_tfidf_corpus = gensim.corpora.MmCorpus('text_tfidf_model_mm')
text_lsi_corpus = gensim.corpora.MmCorpus('text_lsi_model_mm')

print(text_tfidf_corpus)
print(text_lsi_corpus)

# Commented out IPython magic to ensure Python compatibility.
# We will be using MatrixSimilarity to compute cosine similarity against the corpus 

from gensim.similarities import MatrixSimilarity 

# %time text_index = MatrixSimilarity(text_lsi_corpus, num_features = text_lsi_corpus.num_terms)

# SEMANTIC SEARCH TIME
# I HOPE THIS WORKS

from operator import itemgetter

def semantic_search(term):

  query_bow = dictionary.doc2bow(spacy_tokenizer(term))
  query_tfidf = text_tfidf_model[query_bow]
  query_lsi = text_lsi_model[query_tfidf]

  text_index.num_best = 15

  text_list = text_index[query_lsi]

  text_list.sort(key=itemgetter(1), reverse=True)
  best_suggestions = []

  for k, phrase in enumerate(text_list):
    best_suggestions.append (
        {
        'Relevance' : round((phrase[1] * 100), 2),
         'Trend' : df_text['body'][phrase[0]]
        }
    )
    if k == (text_index.num_best-1):
        break

  return pd.DataFrame(best_suggestions, columns=['Relevance', 'Trend'])

semantic_search('iphone')

semantic_search('mag safe')

semantic_search('crab cake')